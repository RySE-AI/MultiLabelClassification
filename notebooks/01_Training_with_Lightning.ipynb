{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completion.use_jedi = False\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "#os.environ[\"MLFLOW_TRACKING_URI\"] = 'http://localhost:5000/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.custom_datasets import MultiLabelDataModule\n",
    "from src.model import MultiLabelClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "1. [General Information](#intro)\n",
    "2. [Experiment Design](#design) <br/>\n",
    "    2.1 [General Settings](#settings) <br/>\n",
    "    2.2 [Transform/Augmentations](#aug) <br/>\n",
    "3. [Experiments](#exp) <br/>\n",
    "    3.1 [Experiment 1 - Backbone pretrained and frozen weights](#id1) <br/>\n",
    "    3.2 [Experiment 2 - Backbone pretrained and unfrozen weights](#id2) <br/>\n",
    "    3.3 [Experiment 3 - Backbone untrained](#id3) <br/>\n",
    "4. [Summary/Evaluation](#eval)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "As mentioned in the readme I'll built a multi-label classifier. So the model can predict more than one class per image (e.g. the color of the card and whether it's a creature or special card).  For this purpose I'll use the binary crossentropy loss with [logits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). This loss treats the outputs/logits independently and is [suitable for multi-label classification](https://discuss.pytorch.org/t/is-there-an-example-for-multi-class-multilabel-classification-in-pytorch/53579/7). For this experiment I'll use a Resnet18 as a backbone. <br/> \n",
    "\n",
    "**_Visual concept:_** <br/>\n",
    "<img src=\"../img/NeuralNetwork_MultiLabel_Concept.svg\" width=500 height=400 align=\"center\"/>\n",
    "\n",
    "**Training:** via pytorch Lightning with torch 2.0.1 - I will use torch.compile  <br/>\n",
    "**Accelerator:** MPS (M1 Max) <br/>\n",
    "**Logger:** MLFlow (local - `mlflow ui --backend-store-uri Coding_Projects/MLFlow_runs`) <br/>\n",
    "**Datamodule:** Custom Module in src (built on the CustomDataset [MultiLabelImageFolder](../src/custom_datasets.py)) <br/>\n",
    "**metrics:** MultiLabel accuracy will be the benchmark. I will also track the Precision and Recall with [MetricCollection](https://torchmetrics.readthedocs.io/en/stable/pages/overview.html?highlight=metriccollection#metriccollection) of torchmetrics <br/>\n",
    "**Callbacks**: ModelCheckpoint, monitoring the validation accuracy <br/>\n",
    "**Profiler**: While Testing I used \"simple\". Really informative profiler which measures the execution time per each action\n",
    "\n",
    "# 2. Experiment Design <a class=\"anchor\" id=\"design\"></a>\n",
    "\n",
    "I'll run 3 different experiments with the same augmentations, image input size and hyperparameters. The backbone will be Resnet18 to keep the model small. The training will run for 50 epochs per each experiment.\n",
    "\n",
    "1. Backbone pretrained frozen\n",
    "2. Backbone pretrained unfrozen\n",
    "3. Backbone untrained unfrozen (normalized on training data)\n",
    "\n",
    "At the end I'll take the best model and will use this model in a streamlit app to visualize the model's inference.\n",
    "\n",
    "#### <u> Attention </u>\n",
    "- If you're planning to make more experiments or want to do hyperparameter tuning you should look for [Ray (Hyperparameter Tuning)](https://docs.ray.io/en/latest/tune/examples/includes/mlflow_ptl_example.html), [papermill (Parameterize Notebooks)](https://papermill.readthedocs.io/en/latest/) or using the [LightningCLI/script parameters](https://lightning.ai/docs/pytorch/stable/common/hyperparameters.html). \n",
    "- Sometimes I had out of memory issues on my Mac where the memory was still allocated after the training. As a temporary solution I'm \"emptying\" the cache after each training to avoid that problem\n",
    "- set persistent_worker to True in your DataLoader. It made the training time much faster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 General Settings <a class=\"anchor\" id=\"settings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFlow Experiment Name\n",
    "PROJECT_NAME = \"Magic The Gathering - Multilabel Classification\"\n",
    "\n",
    "# Other MLFlow related parameters\n",
    "LOCAL_MLFLOW_URI = f\"/Users/ryoshibata/Coding_projects/MLFlow_runs/\"\n",
    "\n",
    "# Local paths where I stored the data\n",
    "data_dirs = {\"train\": \"../data/0.7-0.15-0.15_split/train/\",\n",
    "             \"test\": \"../data/0.7-0.15-0.15_split/test/\",\n",
    "             \"val\": \"../data/0.7-0.15-0.15_split/val/\"}\n",
    "\n",
    "# Experiment Settings\n",
    "batch_size = 32 # with 64 and 4 workers I had memory leak issues \n",
    "hidden_size = 1024\n",
    "lr = 0.001\n",
    "num_classes = 10\n",
    "n_epochs = 50\n",
    "\n",
    "PROFILER = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transform/Augmentations <a class=\"anchor\" id=\"aug\"></a>\n",
    "\n",
    "- I kept the main image shape 445x312 of my dataset (see Image Exploration)\n",
    "    - it's usually recommended to use 224x224 if you use ImageNet trained architectures but I wanted to keep the image ratio of my dataset\n",
    "- I only used the augmentations \"RandomRotation\" and \"RandomHorizontalFlip\" to keep it simple. Nevertheless I tried to improve the performance of the model by adding some randomness\n",
    "    - I will use some real cards for my inference, so that's why I added the rotation augmentation\n",
    "- In the last experiment I changed the mean and std of normalize step to the values I calculated in th exploration notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(degrees=(0, 180)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize((445, 312)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "inference_transform = transforms.Compose([transforms.Resize((445, 312)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                               [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experiments <a class=\"anchor\" id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helperfunction to reduce some code repetitions\n",
    "from typing import Tuple\n",
    "\n",
    "def set_mlflow_and_checkpoint_callback(run_name: str,) -> Tuple[MLFlowLogger, ModelCheckpoint]:\n",
    "    \"\"\"I'm only modifying the run_name for the two instances per each \n",
    "    experiment. It returns the MLFLowLogger and the ModelCheckpoint Callback.\"\"\"\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"./checkpoints/{run_name}/\",\n",
    "        save_top_k=2,\n",
    "        monitor=\"val_MultilabelAccuracy\",\n",
    "        mode=\"max\",\n",
    "    )\n",
    "\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=PROJECT_NAME,\n",
    "        run_name=run_name,\n",
    "        tracking_uri=LOCAL_MLFLOW_URI,\n",
    "        log_model=True,\n",
    "    )\n",
    "\n",
    "    return mlf_logger, checkpoint_callback\n",
    "\n",
    "\n",
    "def set_MultiLabelClassifier(backbone_config: dict):\n",
    "    \"\"\"I'm only modifying the backbones config in this experiment\"\"\"\n",
    "    multilabel_model = MultiLabelClassifier(\n",
    "        backbone_config=backbone_config,\n",
    "        num_classes=num_classes,\n",
    "        hidden_size_1=hidden_size,\n",
    "        hidden_size_2=hidden_size,\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    return multilabel_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Experiment 1 - Backbone pretrained and frozen weights <a class=\"anchor\" id=\"id1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"Experiment_1-Resnet18-pretrained_frozen-weights\"\n",
    "\n",
    "mlf_logger, checkpoint_callback = set_mlflow_and_checkpoint_callback(run_name)\n",
    "\n",
    "mtg_data = MultiLabelDataModule(data_dirs=data_dirs,\n",
    "                                train_transform=train_transform,\n",
    "                                inference_transform=inference_transform,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "mlf_logger.log_hyperparams(train_transform.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ryoshibata/.cache/torch/hub/pytorch_vision_main\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | backbone      | ResnetBackbone    | 11.2 M\n",
      "1 | classifier    | ClassifierHead    | 1.6 M \n",
      "2 | criterion     | BCEWithLogitsLoss | 0     \n",
      "3 | train_metrics | MetricCollection  | 0     \n",
      "4 | valid_metrics | MetricCollection  | 0     \n",
      "5 | test_metrics  | MetricCollection  | 0     \n",
      "----------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "12.8 M    Total params\n",
      "51.047    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 42/42 [00:04<00:00,  8.79it/s, v_num=11cb]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 42/42 [00:04<00:00,  8.63it/s, v_num=11cb]\n"
     ]
    }
   ],
   "source": [
    "# Backbone settings\n",
    "backbone_config = {\n",
    "    \"freeze_params\": True,\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"weights\": \"IMAGENET1K_V1\",\n",
    "}\n",
    "# model settings\n",
    "multilabel_model = set_MultiLabelClassifier(backbone_config)\n",
    "\n",
    "# enable torch 2.x new features compile\n",
    "torch.compile(multilabel_model)\n",
    "\n",
    "# Trainer settings\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    logger=mlf_logger,\n",
    "    accelerator=\"mps\",\n",
    "    profiler=PROFILER,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model=multilabel_model, datamodule=mtg_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/ryoshibata/PycharmProjects/MultiLabelClassification/notebooks/checkpoints/Experiment_1-Resnet18-pretrained_frozen-weights/epoch=49-step=2100.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/ryoshibata/PycharmProjects/MultiLabelClassification/notebooks/checkpoints/Experiment_1-Resnet18-pretrained_frozen-weights/epoch=49-step=2100.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 11.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_MultilabelAccuracy  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9451826810836792     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_MultilabelPrecision  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9019148349761963     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_MultilabelRecall   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8737625479698181     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13455015420913696    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_MultilabelAccuracy \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9451826810836792    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_MultilabelPrecision \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9019148349761963    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_MultilabelRecall  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8737625479698181    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13455015420913696   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.13455015420913696,\n",
       "  'test_MultilabelAccuracy': 0.9451826810836792,\n",
       "  'test_MultilabelPrecision': 0.9019148349761963,\n",
       "  'test_MultilabelRecall': 0.8737625479698181}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(multilabel_model,\n",
    "             datamodule=mtg_data,\n",
    "             ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._C._mps_emptyCache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experiment 2 - Backbone pretrained, not frozen parameters <a class=\"anchor\" id=\"id2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"Experiment_2-Resnet18-pretrained_unfrozen-weights\"\n",
    "\n",
    "mlf_logger, checkpoint_callback = set_mlflow_and_checkpoint_callback(run_name)\n",
    "\n",
    "mlf_logger.log_hyperparams(train_transform.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ryoshibata/.cache/torch/hub/pytorch_vision_main\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | backbone      | ResnetBackbone    | 11.2 M\n",
      "1 | classifier    | ClassifierHead    | 1.6 M \n",
      "2 | criterion     | BCEWithLogitsLoss | 0     \n",
      "3 | train_metrics | MetricCollection  | 0     \n",
      "4 | valid_metrics | MetricCollection  | 0     \n",
      "5 | test_metrics  | MetricCollection  | 0     \n",
      "----------------------------------------------------\n",
      "12.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.8 M    Total params\n",
      "51.047    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 42/42 [00:11<00:00,  3.52it/s, v_num=1ac3]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 42/42 [00:11<00:00,  3.52it/s, v_num=1ac3]\n"
     ]
    }
   ],
   "source": [
    "# Backbone Settings\n",
    "backbone_config = {\n",
    "    \"freeze_params\": False,\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"weights\": \"IMAGENET1K_V1\",\n",
    "}\n",
    "\n",
    "# Model settings\n",
    "multilabel_model = set_MultiLabelClassifier(backbone_config)\n",
    "\n",
    "# enable torch 2.x new features compile\n",
    "torch.compile(multilabel_model)\n",
    "\n",
    "# train model\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    logger=mlf_logger,\n",
    "    accelerator=\"mps\",\n",
    "    profiler=PROFILER,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model=multilabel_model, datamodule=mtg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/ryoshibata/PycharmProjects/MultiLabelClassification/notebooks/checkpoints/Experiment_2-Resnet18-pretrained_unfrozen-weights/epoch=47-step=2016.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/ryoshibata/PycharmProjects/MultiLabelClassification/notebooks/checkpoints/Experiment_2-Resnet18-pretrained_unfrozen-weights/epoch=47-step=2016.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 11.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_MultilabelAccuracy  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9990032911300659     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_MultilabelPrecision  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.999009907245636     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_MultilabelRecall   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9986666440963745     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.005829510744661093    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_MultilabelAccuracy \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9990032911300659    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_MultilabelPrecision \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.999009907245636    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_MultilabelRecall  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9986666440963745    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.005829510744661093   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.005829510744661093,\n",
       "  'test_MultilabelAccuracy': 0.9990032911300659,\n",
       "  'test_MultilabelPrecision': 0.999009907245636,\n",
       "  'test_MultilabelRecall': 0.9986666440963745}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(multilabel_model,\n",
    "             datamodule=mtg_data,\n",
    "             ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._C._mps_emptyCache() # sometimes I had issues with memory leakage and found that solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Experiment 3 - Backbone untrained <a class=\"anchor\" id=\"id3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing mean and std of normalization step\n",
    "train_transform = transforms.Compose([transforms.RandomRotation(degrees=(0, 180)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize((445, 312)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.50476729, 0.48440304, 0.46218942], \n",
    "                                                           [0.30981703, 0.3034715 , 0.30258951])])\n",
    "\n",
    "inference_transform = transforms.Compose([transforms.Resize((445, 312)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.50476729, 0.48440304, 0.46218942],\n",
    "                                                               [0.30981703, 0.3034715 , 0.30258951])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"Experiment_3-Resnet18-untrained\"\n",
    "\n",
    "mlf_logger, checkpoint_callback = set_mlflow_and_checkpoint_callback(run_name)\n",
    "\n",
    "mtg_data = MultiLabelDataModule(data_dirs=data_dirs,\n",
    "                                train_transform=train_transform,\n",
    "                                inference_transform=inference_transform,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "mlf_logger.log_hyperparams(train_transform.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ryoshibata/.cache/torch/hub/pytorch_vision_main\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | backbone      | ResnetBackbone    | 11.2 M\n",
      "1 | classifier    | ClassifierHead    | 1.6 M \n",
      "2 | criterion     | BCEWithLogitsLoss | 0     \n",
      "3 | train_metrics | MetricCollection  | 0     \n",
      "4 | valid_metrics | MetricCollection  | 0     \n",
      "5 | test_metrics  | MetricCollection  | 0     \n",
      "----------------------------------------------------\n",
      "12.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.8 M    Total params\n",
      "51.047    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 42/42 [00:11<00:00,  3.57it/s, v_num=4a0c]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 42/42 [00:11<00:00,  3.57it/s, v_num=4a0c]\n"
     ]
    }
   ],
   "source": [
    "# backbone settings\n",
    "backbone_config = {\n",
    "    \"freeze_params\": False,\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"weights\": None,\n",
    "}\n",
    "# model settings\n",
    "multilabel_model = set_MultiLabelClassifier(backbone_config)\n",
    "\n",
    "# enable torch 2.x new features compile\n",
    "torch.compile(multilabel_model)\n",
    "\n",
    "# Trainer settings\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    logger=mlf_logger,\n",
    "    accelerator=\"mps\",\n",
    "    profiler=PROFILER,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model=multilabel_model, datamodule=mtg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/ryoshibata/PycharmProjects/MultiLabelClassification/notebooks/checkpoints/Experiment_3-Resnet18-untrained/epoch=35-step=1512.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/ryoshibata/PycharmProjects/MultiLabelClassification/notebooks/checkpoints/Experiment_3-Resnet18-untrained/epoch=35-step=1512.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 11.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_MultilabelAccuracy  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9245847463607788     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_MultilabelPrecision  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.845035195350647     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_MultilabelRecall   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8440000414848328     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1341966986656189     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_MultilabelAccuracy \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9245847463607788    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_MultilabelPrecision \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.845035195350647    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_MultilabelRecall  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8440000414848328    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1341966986656189    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.1341966986656189,\n",
       "  'test_MultilabelAccuracy': 0.9245847463607788,\n",
       "  'test_MultilabelPrecision': 0.845035195350647,\n",
       "  'test_MultilabelRecall': 0.8440000414848328}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(multilabel_model,\n",
    "             datamodule=mtg_data,\n",
    "             ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._C._mps_emptyCache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation Summary <a class=\"anchor\" id=\"eval\"></a>\n",
    "\n",
    "By looking at the test metrics (Accuracy, Precision, Recall) the 2nd model is the best performing one. So fine-tuning the whole network led to the best results. In particular the 2nd model outperforms the others in precision and recall. I will use this model for the inference in the next notebook and streamlit app. All metrics including losses of the 2nd experiment are provided [here](./experiment_results/experiment_2/metrics/) (copied from my MLFlow folder). While the training the validation loss was a spiky but with a decreasing trend, maybe for another run it will be better to use a smaller learning rate. The best checkpoint was saved at the 47 epoch(Step=2016).\n",
    "\n",
    "**Table View MLFlow:**<br/><br/>\n",
    "<img src=\"../img/MLFlow_tracking_results_table.png\" width=800 height=180 align=\"center\"/> <br/>\n",
    "\n",
    "**Barcharts MLFlow:**<br/><br/>\n",
    "<img src=\"../img/MLFlow_tracking_results_chart.png\" width=800 height=180 align=\"center\"/>\n",
    "\n",
    "**Experiment 2 Accuracy Train/Val Chart:**<br/><br/>\n",
    "<img src=\"../img/experiment_2_accuracy_train_val_chart.png\" width=800 height=400 align=\"center\"/>\n",
    "\n",
    "**Experiment 2 Loss Train/Val Chart:**<br/><br/>\n",
    "<img src=\"../img/experiment_2_loss_train_val_chart .png\" width=800 height=400 align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step - [Model Inference](./02_Model_Inference.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
